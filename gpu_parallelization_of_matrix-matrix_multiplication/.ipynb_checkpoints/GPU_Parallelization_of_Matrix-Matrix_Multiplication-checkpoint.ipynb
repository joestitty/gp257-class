{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b83b7a",
   "metadata": {},
   "source": [
    "\n",
    "# GPU Numba and CuPy Parallelization of Matrix Multiplication \n",
    "\n",
    "Similary to the multicore parallelization lab, in this lab we will be using Numba and CuPy to accelerate matrix-matrix multiplications using GPU. Accelerating the marrix-matrix multiplication operation is a good analog to accelerating other types of operators and computationally intense kernels, codes, and algorithms. Furthermore, the structure of matricies makes matrix-matrix multiplication a good place start learning how to parallelize code.\n",
    "\n",
    "\n",
    "## External Resources\n",
    "If you have any question regarding some specific Python functionality you can consult the official [Python documenation](http://docs.python.org/3/).\n",
    "\n",
    "* [Numba for CUDA](https://numba.readthedocs.io/en/stable/cuda/index.html)\n",
    "* [Writing Numba.CUDA kernels Notebook](https://github.com/ContinuumIO/gtc2017-numba/blob/master/4%20-%20Writing%20CUDA%20Kernels.ipynb)\n",
    "* [Numba.CUDA by Graham Markell](https://github.com/numba/nvidia-cuda-tutorial)\n",
    "* [NYU Numba CUDA Lab5](https://nyu-cds.github.io/python-numba/05-cuda/)\n",
    "* [CuPy Basics](https://docs.cupy.dev/en/stable/user_guide/basic.html)\n",
    "\n",
    "[//]: <> (GEOPHYS 257 Winter 2023)\n",
    "[//]: <> (Notebook Author: Thomas Cullison, Stanford University, Jan. 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff49d1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 0\n",
    "\n",
    "* You need to request a *T4* node on the cluster. Don't forget that you need to add **--gres=gpu** to your srun command.\n",
    "* Reminder: on the *T4* nodes you need to load a different version of Python:\n",
    "```bash\n",
    "spack load python@3.10.7\n",
    "```\n",
    "\n",
    "* Import every Python module, object, and/or function that you need below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e913f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e9a09",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 1: Matrix Transpose\n",
    "\n",
    "Before we examine matrix-matrix multiplication, we will first write a GPU kernel that transposes a square matrix.  This type of problem is a good introduction into how to use the CUDA threading model. The task for this exercise is to write a Numba CUDA kernel that will transpose a square matrix. \n",
    "\n",
    "**Before you start**, take a look at the following:\n",
    "* Read over the following notebook that explanes Numba.CUDA kernels: [Writing Numba.CUDA kernels Notebook](https://github.com/ContinuumIO/gtc2017-numba/blob/master/4%20-%20Writing%20CUDA%20Kernels.ipynb) \n",
    "* The first matrix-matrix multiplication code (the one that **doesn't** use shared memory) shown at [NYU Numba CUDA Lab5](https://nyu-cds.github.io/python-numba/05-cuda/). Understanding this code should give a pretty good idea on how to write the transpose kernel. The matrix-matrix kernel code from the NYU lab is shown below.\n",
    "```python\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "```\n",
    "\n",
    "**Tasks for this exercise**\n",
    "* Write a Numba.CUDA kernel that transpose an $NxN$ square matrix.\n",
    "* Be sure that the transpose kernel can transpose square matrices with sizes of $N$ as small as $N=2$ and as large as $N=10240$.\n",
    "* Using shared memory is **not** required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7394a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spack/spack/opt/spack/linux-centos7-haswell/gcc-9.5.0/python-3.10.7-xj7xqdemt2mshnxfntrc7bq6nyx6khcv/lib/python3.10/site-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/spack/spack/opt/spack/linux-centos7-haswell/gcc-9.5.0/python-3.10.7-xj7xqdemt2mshnxfntrc7bq6nyx6khcv/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/spack/spack/opt/spack/linux-centos7-haswell/gcc-9.5.0/python-3.10.7-xj7xqdemt2mshnxfntrc7bq6nyx6khcv/lib/python3.10/site-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/spack/spack/opt/spack/linux-centos7-haswell/gcc-9.5.0/python-3.10.7-xj7xqdemt2mshnxfntrc7bq6nyx6khcv/lib/python3.10/site-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 16 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/spack/spack/opt/spack/linux-centos7-haswell/gcc-9.5.0/python-3.10.7-xj7xqdemt2mshnxfntrc7bq6nyx6khcv/lib/python3.10/site-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed the Transpose Test!\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def transpose(A, B):\n",
    "    \"\"\"\n",
    "    Transpose the matrix A and store the result in B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < A.shape[0] and col < A.shape[1]:\n",
    "        B[col, row] = A[row, col]\n",
    "\n",
    "def test_transpose():\n",
    "    # set number of threads per block\n",
    "    threads_per_block = 32\n",
    "    # test matrices of various sizes\n",
    "    for n in [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 10240]: # exp scale\n",
    "        # we can create a random matrix of size nxn\n",
    "        A = np.random.rand(n, n).astype(np.float32)\n",
    "\n",
    "        # allocate memory for the transposed matrix\n",
    "        B = np.zeros((n, n), dtype=np.float32)\n",
    "\n",
    "        blocks_per_grid = (n + threads_per_block - 1) // threads_per_block # need to calculate\n",
    "\n",
    "        # copy the input matrix to the device\n",
    "        d_A = cuda.to_device(A)\n",
    "\n",
    "        # launch the transpose kernel with the appropriate grid and block dimensions\n",
    "        transpose[(blocks_per_grid, blocks_per_grid), (threads_per_block, threads_per_block)](d_A, B)\n",
    "\n",
    "        # I need to copy the result back to the host and verify correctness\n",
    "        assert np.allclose(B, A.T)\n",
    "    \n",
    "    print(\"Passed the Transpose Test!\")\n",
    "        \n",
    "test_transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12070622",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 2: Using Numba CUDA to parallelize matrix multiplication: \n",
    "\n",
    "For this exercise, we will use Numba compiled GPU kernels that calculate matrix-matrix multiplication for square matrices. In particular, we will use a GPU kernel that doesn't used shared memory and compared to a GPU kernel that does use shared-memory. Please use the two kernel codes discussed in the following lab: [NYU Numba CUDA Lab5](https://nyu-cds.github.io/python-numba/05-cuda/). As you will see in this exercise, learning to use shared-memory (akin to user-controlled cache), can take a lot of practice, so in the next exercise, we examine how well the simple shared-memory kernel from the NYU lab compares to the optimized codes provided by NVIDIA in the CuPy package. \n",
    "\n",
    "#### The tasks for this exercise:\n",
    "1. Copy the matrix-matrix kernel codes from the NYU lab. Test them for accurracy against *numpy.dot()* and also compare time runtimes these GPU kernels the numpy.dot() function as well. **Note:** Use [CUDA events](https://numba.readthedocs.io/en/stable/cuda-reference/host.html#events) when timing GPU kernel calls because the driver does not \"block\" the calling process (for case this is IPython). Insted, the kernel is sent to the GPU to run, and then the process (IPython) immediately continues to it's next bit of code. Contrary to GPU kernel calls, calls to copy data to or from the GPU will block the process. For these cases, the calls can be timed the same way that other Python calls are timed.<br> **For both GPU kernels:**\n",
    "    - Test with square Matrices: $A,B \\in \\mathbb{R}^{N\\times N}$. For the cases when $N = 5120$, $N=10240$, and $N=20480$. **Tip**, first make sure you can get the GPU codes to work and that you get correct results by testing with $N_{test}=32$.\n",
    "    - For each $N$ above, test the multiplication for both dtypes: *dtype=float32* and *dtype=float64*.\n",
    "    - Calculate and show the error between your functions and the *numpy.dot()* function. \n",
    "    - Calculate and show the *speedup* (or *slowdown*) of your GPU kernel for each $N$ vs *numpy.dot()*. Be sure to include the array copy times in the \"total-gpu-kernel runtime.\n",
    "    - For each $N$ vs, calculate and show the *speedup* of your GPU kernel using *dtype=float32* vs *dtype=float64*. Be sure to include the array copy times in the \"total-gpu-kernel runtime.\"\n",
    "    \n",
    "<br>\n",
    "\n",
    "2. Create your matrices using random numbers. An example is shown below (feel free to copy this).\n",
    "\n",
    "```python\n",
    "h_A = np.random.random((N, N)).astype(np.<float-type>)\n",
    "h_B = np.random.random((N, N)).astype(np.<float-type>)\n",
    "```    \n",
    "<br>\n",
    "\n",
    "3. For the device memory:\n",
    "    - Create **d_A** and **d_B** by copying **h_A** and **h_B** to the GPU, and be sure to time the copies\n",
    "    - Create **d_C** as device-array that is allocated on the GPU (device) only, and not on the host (**Do Not Copy**)\n",
    "    \n",
    "<br>\n",
    "\n",
    "4. After the GPU matrix-matrix multiplication kernel finishes, **copy** the the *device-array* **d_C** to the *host-array* **h_C**, and be sure to time this copy.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. Discuss your results in the markdown cell that follows your codes include in your discussion remarks about the speedup or slowdowns vs numpy as well as float32 vs float64. Remember, that your runtime for the GPU kernel include time to compile the kernel (not much you can do to control this). Futhermore, becasue you have to copy data to and off of the GPU, these copy times should be included in the \"total-gpu-kernel runtime.\" \n",
    "\n",
    "**Overall, the results show that using GPUs from the cloud for matrix-matrix multiplication with both NumPy and CuPy can provide significant speedups over the CPU implementation. However, we can see that this speedup is highly dependent on the size of the matrices being multiplied. For example, for smaller matrices (e.g., 5120), the NumPy.dot() implementation is actually faster than both the GPU matmul and fast_matmul implementations. I'm not exactly sure why this is true, but I honestly think that it's probably due to the overhead of transferring the data to and from the GPU, which the NumPy developers were able to handle behind the scenes better in the C code. As the matrix size increases, the GPU implementations become increasingly slower. In general, the float32 data type provides better performance than the float64 data type, which is probably due to the lower memory requirements for the float32 data type. We can see that the fast_matmul function provides a slight speedup over the regular matrix multiplication implementation for all the matrix sizes tested. Again, this is due to the shared memory in the kernel, which can actually lead to better memory access for cache utilization as well as memory access patterns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99735477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "# Define block size for shared memory kernel\n",
    "TPB = 32\n",
    "\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B\n",
    "    Each thread computes one element of the result matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=dtype)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=dtype)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(int(A.shape[1] / TPB)):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp\n",
    "    \n",
    "# Define the global variable for dtype\n",
    "dtype = np.float32\n",
    "    \n",
    "def test_matmul():\n",
    "    # In order to test a range of matrix sizes and a range of dtypes \n",
    "    # we might as well loop through them \n",
    "    print(\"-------- Begin Test Mat Mul ----------\")\n",
    "    for n in [5120, 10240, 20480]:\n",
    "        for dtype in [np.float32, np.float64]:\n",
    "            print(f\"Testing Matrix Dim of N={n} and dtype ={dtype}\")\n",
    "            # need to generate numpy arrays that are allocated to host or in memory\n",
    "            h_A = np.random.random((n,n)).astype(dtype)\n",
    "            h_B = np.random.random((n,n)).astype(dtype)\n",
    "            h_C = np.zeros((n,n), dtype=dtype)\n",
    "            # Copy the matrices to the GPU memory to perform matrix matrix mutltiplcation on the GPU\n",
    "            d_A = cuda.to_device(h_A)\n",
    "            d_B = cuda.to_device(h_B)\n",
    "            d_C = cuda.device_array((n,n), dtype=dtype)\n",
    "            # Need to time the GPU kernel \n",
    "            start_event = cuda.event()\n",
    "            end_event = cuda.event()\n",
    "            start_event.record()\n",
    "            # call the matrix multiplication kernel with grid size (32, 32) and block size (32, 32)\n",
    "            matmul[(32,32),(32,32)](d_A,d_B,d_C) # launch cuda kernel \n",
    "            end_event.record() # record completion of event\n",
    "            end_event.synchronize() # block cpu until event is recorded, enure the kernel is finished before moving on\n",
    "            # total time below includes the time of kernel to execute and to copy data to and from device\n",
    "            # value is stored in gpu_time to compare to the np.dot\n",
    "            gpu_time = start_event.elapsed_time(end_event) # calculate total time to execture kernel\n",
    "            # Copy the result back to host or cpu and calculate error\n",
    "            d_C.copy_to_host(h_C)\n",
    "            h_ref = np.dot(h_A, h_B)\n",
    "            error = np.linalg.norm(h_ref - h_C) / np.linalg.norm(h_ref)\n",
    "            print(f\"  matmul GPU kernel time (including array copy time): {gpu_time:.5f} seconds\")\n",
    "            print(f\" Error: {error:.6f}\")\n",
    "            \n",
    "            # Compare with numpy.dot()\n",
    "            np_start_time = time.perf_counter()\n",
    "            np_result = np.dot(h_A, h_B)\n",
    "            np_time = time.perf_counter() - np_start_time\n",
    "            np_error = np.linalg.norm(h_ref - np_result) / np.linalg.norm(h_ref)\n",
    "            print(f\"  numpy.dot() time: {np_time:.5f} seconds\")\n",
    "            print(f\"  Error: {np_error:.6f}\")\n",
    "\n",
    "            # lets calculate speedup/slowdown by comparing the ratios of the times stored\n",
    "            if np_time > gpu_time:\n",
    "                print(f\"  Speedup: {np_time/gpu_time:.2f}x\\n\")\n",
    "            else:\n",
    "                print(f\"  Slowdown: {gpu_time/np_time:.2f}x\\n\")\n",
    "\n",
    "def test_fast_matmul():\n",
    "    # In order to test a range of matrix sizes and a range of dtypes \n",
    "    # we might as well loop through them \n",
    "    print(\"-------- Begin Test Fast Mat Mul ----------\")\n",
    "    for n in [5120, 10240, 20480]:\n",
    "        for my_dtype in [np.float32, np.float64]:\n",
    "            print(f\"Testing Matrix Dim of N={n} and dtype ={my_dtype}\")\n",
    "            # need to generate numpy arrays that are allocated to host or in memory\n",
    "            h_A = np.random.random((n,n)).astype(my_dtype)\n",
    "            h_B = np.random.random((n,n)).astype(my_dtype)\n",
    "            h_C = np.empty((n,n), dtype=my_dtype)\n",
    "            # copy the matrices to the GPU memory to perform matrix matrix mutltiplcation on the GPU\n",
    "            d_A = cuda.to_device(h_A)\n",
    "            d_B = cuda.to_device(h_B)\n",
    "            d_C = cuda.device_array((n,n), dtype=my_dtype)\n",
    "            # need to time the GPU kernel \n",
    "            start_event = cuda.event()\n",
    "            end_event = cuda.event()\n",
    "            start_event.record()\n",
    "            # call the fast matrix multiplication kernel with grid size (n/TPB, n/TPB) and block size (TPB, TPB)\n",
    "            fast_matmul[(32,32),(32,32)](d_A,d_B,d_C) # launch cuda kernel \n",
    "            end_event.record() # record completion of event\n",
    "            end_event.synchronize() # block cpu until event is recorded, ensure the kernel is finished before moving on\n",
    "            # total time below includes the time of kernel to execute and to copy data to and from device\n",
    "            # value is stored in gpu_time to compare to the np.dot\n",
    "            gpu_time = start_event.elapsed_time(end_event) # calculate total time to execute kernel\n",
    "            # Copy the result back to host or cpu and calculate error\n",
    "            d_C.copy_to_host(h_C)\n",
    "            h_ref = np.dot(h_A, h_B)\n",
    "            error = np.linalg.norm(h_ref - h_C) / np.linalg.norm(h_ref)\n",
    "            print(f\"  fast_matmul GPU kernel time (including array copy time): {gpu_time:.5f} seconds\")\n",
    "            print(f\" Error: {error:.6f}\")\n",
    "\n",
    "            # I need to compare with numpy.dot()\n",
    "            np_start_time = time.perf_counter()\n",
    "            np_result = np.dot(h_A, h_B)\n",
    "            np_time = time.perf_counter() - np_start_time\n",
    "            np_error = np.linalg.norm(h_ref - np_result) / np.linalg.norm(h_ref)\n",
    "            print(f\"  numpy.dot() time: {np_time:.5f} seconds\")\n",
    "            print(f\"  Error: {np_error:.6f}\")\n",
    "\n",
    "            # calculate speedup and slowdown by comparing the ratios of the times stored\n",
    "            if np_time > gpu_time:\n",
    "                print(f\"  Speedup: {np_time/gpu_time:.2f}x\\n\")\n",
    "            else:\n",
    "                print(f\"  Slowdown: {gpu_time/np_time:.2f}x\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab42a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Begin Test Mat Mul ----------\n",
      "Testing Matrix Dim of N=5120 and dtype =<class 'numpy.float32'>\n",
      "  matmul GPU kernel time (including array copy time): 685.33807 seconds\n",
      " Error: 0.979811\n",
      "  numpy.dot() time: 0.58103 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 1179.52x\n",
      "\n",
      "Testing Matrix Dim of N=5120 and dtype =<class 'numpy.float64'>\n",
      "  matmul GPU kernel time (including array copy time): 472.10748 seconds\n",
      " Error: 0.979800\n",
      "  numpy.dot() time: 1.18446 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 398.58x\n",
      "\n",
      "Testing Matrix Dim of N=10240 and dtype =<class 'numpy.float32'>\n",
      "  matmul GPU kernel time (including array copy time): 546.39929 seconds\n",
      " Error: 0.995059\n",
      "  numpy.dot() time: 3.98202 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 137.22x\n",
      "\n",
      "Testing Matrix Dim of N=10240 and dtype =<class 'numpy.float64'>\n",
      "  matmul GPU kernel time (including array copy time): 703.17706 seconds\n",
      " Error: 0.994986\n",
      "  numpy.dot() time: 10.40251 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 67.60x\n",
      "\n",
      "Testing Matrix Dim of N=20480 and dtype =<class 'numpy.float32'>\n",
      "  matmul GPU kernel time (including array copy time): 1114.38184 seconds\n",
      " Error: 0.998606\n",
      "  numpy.dot() time: 32.41801 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 34.38x\n",
      "\n",
      "Testing Matrix Dim of N=20480 and dtype =<class 'numpy.float64'>\n",
      "  matmul GPU kernel time (including array copy time): 1944.96521 seconds\n",
      " Error: 0.998749\n",
      "  numpy.dot() time: 63.25174 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 30.75x\n",
      "\n",
      "-------- Begin Test Fast Mat Mul ----------\n",
      "Testing Matrix Dim of N=5120 and dtype =<class 'numpy.float32'>\n",
      "  fast_matmul GPU kernel time (including array copy time): 391.13196 seconds\n",
      " Error: 0.979784\n",
      "  numpy.dot() time: 0.55327 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 706.95x\n",
      "\n",
      "Testing Matrix Dim of N=5120 and dtype =<class 'numpy.float64'>\n",
      "  fast_matmul GPU kernel time (including array copy time): 524.99493 seconds\n",
      " Error: 0.979791\n",
      "  numpy.dot() time: 1.13950 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 460.72x\n",
      "\n",
      "Testing Matrix Dim of N=10240 and dtype =<class 'numpy.float32'>\n",
      "  fast_matmul GPU kernel time (including array copy time): 543.41473 seconds\n",
      " Error: 0.995075\n",
      "  numpy.dot() time: 3.92203 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 138.55x\n",
      "\n",
      "Testing Matrix Dim of N=10240 and dtype =<class 'numpy.float64'>\n",
      "  fast_matmul GPU kernel time (including array copy time): 525.33008 seconds\n",
      " Error: 0.994993\n",
      "  numpy.dot() time: 8.23021 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 63.83x\n",
      "\n",
      "Testing Matrix Dim of N=20480 and dtype =<class 'numpy.float32'>\n",
      "  fast_matmul GPU kernel time (including array copy time): 903.93762 seconds\n",
      " Error: 0.998607\n",
      "  numpy.dot() time: 30.36639 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 29.77x\n",
      "\n",
      "Testing Matrix Dim of N=20480 and dtype =<class 'numpy.float64'>\n",
      "  fast_matmul GPU kernel time (including array copy time): 798.35504 seconds\n",
      " Error: 0.998748\n",
      "  numpy.dot() time: 63.15624 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 12.64x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_matmul()\n",
    "test_fast_matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f35012",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 3: CuPy \n",
    "\n",
    "For this exercise, we will repeat what we did in *Exercise 2*. However, we will use *CuPy* functions, which are similar to *Numpy* funcstions with some added functions for copying data to-the-device-from-the-host and to-the-host-from-the-device. By using CuPy, we can depend on code that has been optimized for the GPU by NVIDIA, and instead of tyring to optimize our matrix-matrix multiplication kernels, we can use a built-in function to calculate the multiplication instead (i.e. [cupy.dot()](https://docs.cupy.dev/en/stable/reference/generated/cupy.dot.html#cupy.dot)).\n",
    "\n",
    "**Tasks for this exercise:**\n",
    "* Same as those listed in *Exercise 2*, but compare *cupy.dot()* to *numpy.dot()*.\n",
    "* Also, reuse the host-arrays, *h_A* and *h_B* above. You will need to call the appropriate *CuPy* fuctions to copy these arrays to the GPU and to copy the result back to the host. You will **not** need to declare the deive-C array before calling *cupy.dot()* because the function will do it for you (like numpy does).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d6103d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "def test_cupy_dot():\n",
    "    print(\"-------- Begin Test CuPy Dot ----------\")\n",
    "    for n in [5120, 10240, 20480]:\n",
    "        for my_dtype in [np.float32, np.float64]:\n",
    "            print(f\"Testing Matrix Dim of N={n} and dtype ={my_dtype}\")\n",
    "            h_A = np.random.random((n,n)).astype(my_dtype) # generate a random matrix on the CPU\n",
    "            h_B = np.random.random((n,n)).astype(my_dtype)\n",
    "            d_A = cp.asarray(h_A)\n",
    "            d_B = cp.asarray(h_B)\n",
    "            start_event = cp.cuda.Event()\n",
    "            end_event = cp.cuda.Event()\n",
    "            start_event.record()\n",
    "            # use the cupy dot operator for a speed boost!\n",
    "            d_C = cp.dot(d_A, d_B)\n",
    "            end_event.record()\n",
    "            end_event.synchronize()\n",
    "            gpu_time = cp.cuda.get_elapsed_time(start_event, end_event)\n",
    "            h_C = d_C.get()\n",
    "            h_ref = np.dot(h_A, h_B)\n",
    "            error = np.linalg.norm(h_ref - h_C) / np.linalg.norm(h_ref)\n",
    "            print(f\"  CuPy dot() time (including array copy time): {gpu_time:.5f} seconds\")\n",
    "            print(f\"  Error: {error:.6f}\")\n",
    "            # generate a random matrix on the CPU\n",
    "            np_start_time = time.perf_counter()\n",
    "            # calculate the result using NumPy\n",
    "            np_result = np.dot(h_A, h_B)\n",
    "            np_time = time.perf_counter() - np_start_time\n",
    "            np_error = np.linalg.norm(h_ref - np_result) / np.linalg.norm(h_ref)\n",
    "            print(f\"  numpy.dot() time: {np_time:.5f} seconds\")\n",
    "            print(f\"  Error: {np_error:.6f}\")\n",
    "            if np_time > gpu_time:\n",
    "                print(f\"  Speedup: {np_time/gpu_time:.2f}x\\n\")\n",
    "            else:\n",
    "                print(f\"  Slowdown: {gpu_time/np_time:.2f}x\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d64d822c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Begin Test CuPy Dot ----------\n",
      "Testing Matrix Dim of N=5120 and dtype =<class 'numpy.float32'>\n",
      "  CuPy dot() time (including array copy time): 91.18311 seconds\n",
      "  Error: 0.000001\n",
      "  numpy.dot() time: 0.59898 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 152.23x\n",
      "\n",
      "Testing Matrix Dim of N=5120 and dtype =<class 'numpy.float64'>\n",
      "  CuPy dot() time (including array copy time): 1237.68579 seconds\n",
      "  Error: 0.000000\n",
      "  numpy.dot() time: 1.19471 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 1035.97x\n",
      "\n",
      "Testing Matrix Dim of N=10240 and dtype =<class 'numpy.float32'>\n",
      "  CuPy dot() time (including array copy time): 596.66364 seconds\n",
      "  Error: 0.000002\n",
      "  numpy.dot() time: 4.08636 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 146.01x\n",
      "\n",
      "Testing Matrix Dim of N=10240 and dtype =<class 'numpy.float64'>\n",
      "  CuPy dot() time (including array copy time): 8603.12207 seconds\n",
      "  Error: 0.000000\n",
      "  numpy.dot() time: 8.46728 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 1016.04x\n",
      "\n",
      "Testing Matrix Dim of N=20480 and dtype =<class 'numpy.float32'>\n",
      "  CuPy dot() time (including array copy time): 4764.24854 seconds\n",
      "  Error: 0.000002\n",
      "  numpy.dot() time: 31.27813 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 152.32x\n",
      "\n",
      "Testing Matrix Dim of N=20480 and dtype =<class 'numpy.float64'>\n",
      "  CuPy dot() time (including array copy time): 70084.93750 seconds\n",
      "  Error: 0.000000\n",
      "  numpy.dot() time: 65.37638 seconds\n",
      "  Error: 0.000000\n",
      "  Slowdown: 1072.02x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cupy_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e1dfb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise $\\mathbf{\\pi}$: CuPy Interoperability\n",
    "\n",
    "Numba and CuPy device arrays (GPU arrays) can be accept each other's arrays. See [Interoperability](https://docs.cupy.dev/en/stable/user_guide/interoperability.html).\n",
    "\n",
    "**Tasks for this exercise**\n",
    "* Use the **device** arrays, **d_A** and **d_B**, that were created in *Exercise 2* to calculate the matrix-matrix multiplcation using *cupy.dot()*.\n",
    "* Verify that you get the same results as you did in *Exercise 3*.\n",
    "* You will need to \"wrap\" the device arrays before passing them to *cupy.dot()*. Read the *Interoperability* documentation linked above.\n",
    "    - Time how long it takes (runtime) to \"wrap\" these arrays.\n",
    "    - Compare this runtime to the runtime it took to create the device arrays in *Exercise 3*.\n",
    "    - Provide a quick comment your thoughts on the runtime differences compared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45445fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Begin Exercise 4 ----------\n",
      "Testing Matrix Dim of N=5120 and dtype =<class 'numpy.float32'>\n",
      "Cupy dot result:\n",
      "[[1275.6204 1282.4775 1284.7528 ... 1289.4395 1267.918  1289.3221]\n",
      " [1249.1144 1265.7539 1263.227  ... 1275.1499 1249.0903 1273.4601]\n",
      " [1251.5381 1262.9202 1266.4703 ... 1284.3322 1254.256  1260.6201]\n",
      " ...\n",
      " [1253.734  1276.6754 1280.9144 ... 1280.2214 1268.2242 1270.5541]\n",
      " [1239.2091 1267.2832 1261.0264 ... 1269.3296 1242.2601 1263.4401]\n",
      " [1258.0967 1268.5521 1264.59   ... 1266.8036 1270.1566 1270.0197]]\n",
      "\n",
      "Reference numpy dot result:\n",
      "[[1275.6177 1282.4773 1284.751  ... 1289.439  1267.9171 1289.3198]\n",
      " [1249.1133 1265.7535 1263.2258 ... 1275.149  1249.0896 1273.4614]\n",
      " [1251.5365 1262.9214 1266.4703 ... 1284.332  1254.2543 1260.6207]\n",
      " ...\n",
      " [1253.7339 1276.6758 1280.9146 ... 1280.2217 1268.2239 1270.5546]\n",
      " [1239.2107 1267.2839 1261.0269 ... 1269.3309 1242.2606 1263.4401]\n",
      " [1258.0967 1268.5532 1264.5894 ... 1266.8025 1270.1587 1270.0204]]\n",
      "\n",
      "Error: 0.000001\n",
      "Time to wrap device arrays: 0.63054 seconds\n",
      "Time to perform matrix multiplication: 0.63054 seconds\n",
      "------------------------------\n",
      "\n",
      "Testing Matrix Dim of N=5120 and dtype =<class 'numpy.float64'>\n",
      "Cupy dot result:\n",
      "[[1287.74865286 1285.57442103 1285.80037136 ... 1297.9526105\n",
      "  1308.63487573 1298.59012599]\n",
      " [1279.68671976 1270.75514846 1260.29620614 ... 1271.68385745\n",
      "  1275.42496798 1276.11410871]\n",
      " [1279.01481528 1275.95576591 1273.55523262 ... 1276.95688319\n",
      "  1289.28544476 1275.7588634 ]\n",
      " ...\n",
      " [1294.41169348 1290.45230389 1272.47401256 ... 1304.88743256\n",
      "  1300.61420745 1301.10335208]\n",
      " [1283.49558181 1284.93978316 1268.79171777 ... 1285.37725552\n",
      "  1294.2046503  1288.86949034]\n",
      " [1268.52717061 1271.07121715 1259.56956607 ... 1279.55481451\n",
      "  1291.34437989 1276.8422582 ]]\n",
      "\n",
      "Reference numpy dot result:\n",
      "[[1287.74865286 1285.57442103 1285.80037136 ... 1297.9526105\n",
      "  1308.63487573 1298.59012599]\n",
      " [1279.68671976 1270.75514846 1260.29620614 ... 1271.68385745\n",
      "  1275.42496798 1276.11410871]\n",
      " [1279.01481528 1275.95576591 1273.55523262 ... 1276.95688319\n",
      "  1289.28544476 1275.7588634 ]\n",
      " ...\n",
      " [1294.41169348 1290.45230389 1272.47401256 ... 1304.88743256\n",
      "  1300.61420745 1301.10335208]\n",
      " [1283.49558181 1284.93978316 1268.79171777 ... 1285.37725552\n",
      "  1294.2046503  1288.86949034]\n",
      " [1268.52717061 1271.07121715 1259.56956607 ... 1279.55481451\n",
      "  1291.34437989 1276.8422582 ]]\n",
      "\n",
      "Error: 0.000000\n",
      "Time to wrap device arrays: 0.00097 seconds\n",
      "Time to perform matrix multiplication: 0.00097 seconds\n",
      "------------------------------\n",
      "\n",
      "Testing Matrix Dim of N=10240 and dtype =<class 'numpy.float32'>\n",
      "Cupy dot result:\n",
      "[[2564.341  2557.8845 2592.7063 ... 2558.2263 2583.008  2553.7053]\n",
      " [2546.5369 2547.7717 2564.8174 ... 2553.9785 2557.6472 2531.2085]\n",
      " [2555.2395 2546.314  2566.4294 ... 2542.826  2565.0793 2535.7107]\n",
      " ...\n",
      " [2552.3052 2558.3533 2565.4087 ... 2544.0466 2558.9766 2515.8884]\n",
      " [2579.066  2573.1619 2595.9153 ... 2579.9106 2580.6514 2549.408 ]\n",
      " [2556.4119 2577.8176 2586.4712 ... 2565.0881 2583.3926 2564.6091]]\n",
      "\n",
      "Reference numpy dot result:\n",
      "[[2564.3357 2557.876  2592.707  ... 2558.228  2583.0042 2553.7017]\n",
      " [2546.5352 2547.7727 2564.8132 ... 2553.9788 2557.6497 2531.2124]\n",
      " [2555.2385 2546.3132 2566.4255 ... 2542.8237 2565.085  2535.7043]\n",
      " ...\n",
      " [2552.3113 2558.3591 2565.408  ... 2544.0454 2558.9717 2515.8923]\n",
      " [2579.0623 2573.1643 2595.9216 ... 2579.911  2580.65   2549.4148]\n",
      " [2556.4133 2577.8257 2586.475  ... 2565.0908 2583.391  2564.6152]]\n",
      "\n",
      "Error: 0.000002\n",
      "Time to wrap device arrays: 0.00098 seconds\n",
      "Time to perform matrix multiplication: 0.00098 seconds\n",
      "------------------------------\n",
      "\n",
      "Testing Matrix Dim of N=10240 and dtype =<class 'numpy.float64'>\n",
      "Cupy dot result:\n",
      "[[2548.91261248 2527.53321802 2535.45497244 ... 2565.34919676\n",
      "  2552.72863069 2512.2497161 ]\n",
      " [2548.08075914 2532.48811379 2542.35617135 ... 2560.41361235\n",
      "  2574.99275623 2526.77093863]\n",
      " [2588.99894303 2557.64868126 2586.88438148 ... 2593.97537345\n",
      "  2596.1287107  2573.91941112]\n",
      " ...\n",
      " [2587.94249933 2530.50661113 2592.40156568 ... 2580.68389983\n",
      "  2590.89349424 2559.16790325]\n",
      " [2534.25251213 2511.65389576 2548.28976757 ... 2560.46231718\n",
      "  2540.57853248 2525.6003316 ]\n",
      " [2547.44045498 2524.77615989 2554.85441246 ... 2561.62955711\n",
      "  2580.37840792 2543.89654853]]\n",
      "\n",
      "Reference numpy dot result:\n",
      "[[2548.91261248 2527.53321802 2535.45497244 ... 2565.34919676\n",
      "  2552.72863069 2512.2497161 ]\n",
      " [2548.08075914 2532.48811379 2542.35617135 ... 2560.41361235\n",
      "  2574.99275623 2526.77093863]\n",
      " [2588.99894303 2557.64868126 2586.88438148 ... 2593.97537345\n",
      "  2596.1287107  2573.91941112]\n",
      " ...\n",
      " [2587.94249933 2530.50661113 2592.40156568 ... 2580.68389983\n",
      "  2590.89349424 2559.16790325]\n",
      " [2534.25251213 2511.65389576 2548.28976757 ... 2560.46231718\n",
      "  2540.57853248 2525.6003316 ]\n",
      " [2547.44045498 2524.77615989 2554.85441246 ... 2561.62955711\n",
      "  2580.37840792 2543.89654853]]\n",
      "\n",
      "Error: 0.000000\n",
      "Time to wrap device arrays: 0.00136 seconds\n",
      "Time to perform matrix multiplication: 0.00136 seconds\n",
      "------------------------------\n",
      "\n",
      "Testing Matrix Dim of N=20480 and dtype =<class 'numpy.float32'>\n",
      "Cupy dot result:\n",
      "[[5113.317  5125.9526 5162.1772 ... 5133.65   5168.667  5151.7983]\n",
      " [5149.25   5149.8354 5192.024  ... 5173.4834 5193.2183 5188.946 ]\n",
      " [5147.269  5124.7583 5149.332  ... 5171.972  5179.3423 5160.7856]\n",
      " ...\n",
      " [5146.645  5144.5425 5156.294  ... 5149.072  5174.7637 5171.3477]\n",
      " [5109.3154 5105.232  5137.8486 ... 5112.468  5147.714  5141.5503]\n",
      " [5092.7007 5074.117  5113.697  ... 5095.786  5118.662  5115.3525]]\n",
      "\n",
      "Reference numpy dot result:\n",
      "[[5113.3276 5125.958  5162.1675 ... 5133.653  5168.671  5151.7954]\n",
      " [5149.274  5149.8193 5192.0146 ... 5173.4917 5193.232  5188.946 ]\n",
      " [5147.289  5124.7603 5149.3335 ... 5171.9673 5179.3335 5160.7695]\n",
      " ...\n",
      " [5146.635  5144.528  5156.295  ... 5149.0547 5174.747  5171.3486]\n",
      " [5109.3003 5105.228  5137.8647 ... 5112.463  5147.6924 5141.5386]\n",
      " [5092.712  5074.1147 5113.681  ... 5095.787  5118.6484 5115.3486]]\n",
      "\n",
      "Error: 0.000002\n",
      "Time to wrap device arrays: 0.00233 seconds\n",
      "Time to perform matrix multiplication: 0.00233 seconds\n",
      "------------------------------\n",
      "\n",
      "Testing Matrix Dim of N=20480 and dtype =<class 'numpy.float64'>\n",
      "Cupy dot result:\n",
      "[[5128.34353519 5109.36066446 5075.69287515 ... 5113.97616625\n",
      "  5071.62427661 5125.26771351]\n",
      " [5167.80694385 5166.64482292 5130.24071157 ... 5178.13817461\n",
      "  5154.19586712 5177.93347789]\n",
      " [5165.62324922 5122.39873847 5123.99892505 ... 5177.07115129\n",
      "  5146.53515842 5145.28867427]\n",
      " ...\n",
      " [5153.3843601  5153.71645    5097.66495268 ... 5179.07215873\n",
      "  5124.73561614 5137.58458135]\n",
      " [5146.18436425 5127.87884044 5089.15641205 ... 5137.6142445\n",
      "  5118.25145024 5110.19898618]\n",
      " [5176.17575366 5122.0790253  5101.43140162 ... 5163.46646771\n",
      "  5096.47602115 5145.34977316]]\n",
      "\n",
      "Reference numpy dot result:\n",
      "[[5128.34353519 5109.36066446 5075.69287515 ... 5113.97616625\n",
      "  5071.62427661 5125.26771351]\n",
      " [5167.80694385 5166.64482292 5130.24071157 ... 5178.13817461\n",
      "  5154.19586712 5177.93347789]\n",
      " [5165.62324922 5122.39873847 5123.99892505 ... 5177.07115129\n",
      "  5146.53515842 5145.28867427]\n",
      " ...\n",
      " [5153.3843601  5153.71645    5097.66495268 ... 5179.07215873\n",
      "  5124.73561614 5137.58458135]\n",
      " [5146.18436425 5127.87884044 5089.15641205 ... 5137.6142445\n",
      "  5118.25145024 5110.19898618]\n",
      " [5176.17575366 5122.0790253  5101.43140162 ... 5163.46646771\n",
      "  5096.47602115 5145.34977316]]\n",
      "\n",
      "Error: 0.000000\n",
      "Time to wrap device arrays: 0.00773 seconds\n",
      "Time to perform matrix multiplication: 0.00773 seconds\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cupy as cp\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "# here I need to define the global variable for dtype\n",
    "dtype = np.float32\n",
    "\n",
    "def test_matmul(d_A, d_B):\n",
    "    # Wrap the device arrays\n",
    "    wrapped_d_A = cp.asarray(d_A)\n",
    "    wrapped_d_B = cp.asarray(d_B)\n",
    "\n",
    "    # Calculate matrix multiplication using cupy.dot()\n",
    "    start_wrap = time.time()\n",
    "    result = cp.dot(wrapped_d_A, wrapped_d_B)\n",
    "    end_wrap = time.time()\n",
    "\n",
    "    # copy result back to host memory\n",
    "    h_result = result.get()\n",
    "\n",
    "    # compare with reference numpy result\n",
    "    h_ref = np.dot(d_A.copy_to_host(), d_B.copy_to_host())\n",
    "    error = np.linalg.norm(h_ref - h_result) / np.linalg.norm(h_ref)\n",
    "\n",
    "    # Print results\n",
    "    # print(f\"Cupy dot result:\\n{h_result}\\n\")\n",
    "    # print(f\"Reference numpy dot result:\\n{h_ref}\\n\")\n",
    "    print(f\"Error: {error:.6f}\")\n",
    "    print(f\"Time to wrap device arrays: {end_wrap - start_wrap:.5f} seconds\")\n",
    "    print(f\"Time to perform matrix multiplication: {end_wrap - start_wrap:.5f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "# separate test function for Exercise 4 that utilizes and mimics the same device setting as exercise 2\n",
    "def test_exercise_4():\n",
    "    # In order to test a range of matrix sizes and a range of dtypes \n",
    "    # we might as well loop through them \n",
    "    print(\"-------- Begin Exercise 4 ----------\")\n",
    "    for n in [5120, 10240, 20480]:\n",
    "        for dtype in [np.float32, np.float64]:\n",
    "            print(f\"Testing Matrix Dim of N={n} and dtype ={dtype}\")\n",
    "            # need to generate numpy arrays that are allocated to host or in memory\n",
    "            h_A = np.random.random((n,n)).astype(dtype)\n",
    "            h_B = np.random.random((n,n)).astype(dtype)\n",
    "            # copy the matrices to the GPU memory to perform matrix matrix multiplication on the GPU\n",
    "            # use the same type of device arrays from exersize 2!\n",
    "            d_A = cuda.to_device(h_A)\n",
    "            d_B = cuda.to_device(h_B)\n",
    "\n",
    "            #  test_matmul function to wrap device arrays and perform matrix multiplication\n",
    "            test_matmul(d_A, d_B)\n",
    "\n",
    "            print(\"------------------------------\\n\")\n",
    "            \n",
    "test_exercise_4()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11fd839",
   "metadata": {},
   "source": [
    "**Looking at the results between Exercise 2, 3, and 4, the runtimes for Exercise 4 are definitely significantly lower than those of Exercises 2 and 3.In Exercise 2 we perform matrix multiplication using the GPU kernel and produces the slowest runtimes among the three exercises. It is roughly 1179.52 times slower for N = 5120 with dtype = float32 and 30.75 times slower for N = 20480 with dtype = float64 when compared to numpy's dot function. When running exercise 3, it utilizes CuPy's dot function and performs faster than Exercise 2, but its still slower than Exercise 4. It is roughly 152.23 times slower for N = 5120 with dtype = float32 and 1072.02 times slower for N = 20480 with dtype = float64 when compared to numpy's dot function. When performing Exercise 4 utilizes CuPy's Interoperability, it generates the fastest runtimes among the three exercises. This is because its using the CuPy library which is faster than the numba wrapped funcitons in exersize 2, but it also wraps the device arrays and performs matrix multiplication. Thus, comparing to exersize 3, test_cupy_dot() includes the time it takes to copy the numpy arrays to the GPU memory and copy the result back to the host memory, while test_matmul() only measures the time it takes to wrap the device arrays and perform the matrix multiplication on the GPU. This makes test_cupy_dot() slower than test_matmul(). It is roughly 706.95 times slower for N = 5120 with dtype = float32 and 12.64 times slower for N = 20480 with dtype = float64 when compared to numpy's dot function.**\n",
    "\n",
    "| Exercise | Matrix Dim | Data Type | CuPy/CUDA Time (s) | CuPy/CUDA Error | NumPy Time (s) | NumPy Error | Slowdown |\n",
    "|----------|-----------|-----------|--------------------|------------------|----------------|-------------|----------|\n",
    "| 2        | 5120      | float32   | 685.34             | 0.979811         | 0.58103        | 0.0         | 1179.52x |\n",
    "| 2        | 5120      | float64   | 472.11             | 0.979800         | 1.18446        | 0.0         | 398.58x  |\n",
    "| 2        | 10240     | float32   | 546.40             | 0.995059         | 3.98202        | 0.0         | 137.22x  |\n",
    "| 2        | 10240     | float64   | 703.18             | 0.994986         | 10.40251       | 0.0         | 67.60x   |\n",
    "| 2        | 20480     | float32   | 1114.38            | 0.998606         | 32.41801       | 0.0         | 34.38x   |\n",
    "| 2        | 20480     | float64   | 1944.97            | 0.998749         | 63.25174       | 0.0         | 30.75x   |\n",
    "| 3        | 5120      | float32   | 91.18              | 0.000001         | 0.59898        | 0.0         | 152.23x  |\n",
    "| 3        | 5120      | float64   | 1237.69            | 0.000000         | 1.19471        | 0.0         | 1035.97x |\n",
    "| 3        | 10240     | float32   | 596.66             | 0.000002         | 4.08636        | 0.0         | 146.01x  |\n",
    "| 3        | 10240     | float64   | 8603.12            | 0.000000         | 8.46728        | 0.0         | 1016.04x |\n",
    "| 3        | 20480     | float32   | 4764.25            | 0.000002         | 31.27813       | 0.0         | 152.32x  |\n",
    "| 3        | 20480     | float64   | 70084.94           | 0.000000         | 65.37638       | 0.0         | 1072.02x |\n",
    "| 4        | 5120      | float32   | 0.63               | 0.000001         | N/A            | N/A         | N/A      |\n",
    "| 4        | 5120      | float64   | 0.001              | 0.000000         | N/A            | N/A         | N/A      |\n",
    "| 4        | 10240     | float32   | 0.001              | 0.000002         | N/A            | N/A         | N/A      |\n",
    "| 4        | 10240      | float64   | 0.001               | 0.000000         | N/A            | N/A         | N/A      |\n",
    "| 4        | 20480     | float32   | 0.002              | 0.000000         | N/A            | N/A         | N/A      |\n",
    "| 4        | 20480    | float64   | 0.002              | 0.000000        | N/A            | N/A         | N/A      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439bd20f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
